# agentic_ai

This is a repo that I showcase what I learned from [AI Engineer Agentic Track: The Complete Agent & MCP Course](https://www.udemy.com/course/the-complete-agentic-ai-engineering-course/) and Agentic AI applications deployed in [HuggingFace](https://HuggingFace.co) in this course

* [Career Conversation](https://huggingface.co/spaces/threecuptea/career_conversation_sonya): Consider it as an interactive Sonya's resume/ profile. It's a [Gradio](https://gradio.app/) ChatBatbot application that can engage coversations with users and pass questions to OpenAI GPT's LLM model that would reply given my Linked profile and summary as the context.  It also interfaces with [Pushover](https://pushover.net/) text notification. I registered two customized Agent tool functions 'record_user_details' and 'record_unknown_question' with OpenAI and give instruction to encourage users to leave email address and call tool 'record_user_details' with the email given and call 'record_unknown_question' to record questions if the model is unable to answer.  Both tool functions will notify me via text message.  The design and prompts are well thought out and is very applicable.
* [Deep Research v2](https://huggingface.co/spaces/threecuptea/deep_research2): It's an improved version of [Deep Research with OpenAI SDK](https://github.com/ed-donner/agents/tree/main/2_openai/deep_research) - A simple multi-agent app. Research manager coordinates with plan_agent: coming up with 3 search terms for the query given; search_agent: searching with [OpenAI WebSearch Tool](https://platform.openai.com/docs/guides/tools-web-search); write_agent: cominng up the outline and writing report in markdown format based upon the requirement; email_agent: coming up the subject and converting the report to HTML format to send to the hard-coded designated email. I added features to send a copy to the email given upon request as well as verify the email using an input guardrail.  I have to pass multiple inputs to the email agent. The process of searching for the answer (ends up passing an email prompt) reminds me that LLM is behind the scenes.
  Lesson learned and things to improve: LLM is non-deterministic and need to learn context engineering so that the result can be more predictable.  Add evaluator agenet(s) in important steps to ensure the quality.  Of course, API rate limit, so that those apps won't break my budget!!
* [Trip Daily Planner](https://huggingface.co/spaces/threecuptea/trip_daily_planner): It's a CrewAI Gradio application that will get traveler's necessary information: destination, flight, hotel location, trip duration, personal interests, dining restrictions and preferences and email from Gradio UI. The application is using very popular [SerperDevTool](https://docs.crewai.com/en/tools/search-research/serperdevtool), [ScrapeWebsiteTool](https://crewai.com/en/tools/web-scraping/scrapewebsitetool) and would plan daily activities and restaurants for the trip and send daily planner to the email provided. I learned the following lessons:

  How to overcome a common rate limit error: exceeds 30,000 tokens per minute. I get it working with the following Agent and SerperDevTool parameters. The most important parameter is SerperDevTool's n_results that decides numbers of tokens parsed in one run. It fails right away if I bump it up to 5.

  Agent(
  config=self.agents_config['personalized_activity_planner'],
  tools=[SerperDevTool(n_results=3), ScrapeWebsiteTool()],
  max_retry_limit=1,
  max_iter=2,
  respect_context_window=True,
  verbose=True,
  allow_delegation=False,
  )

  One advantage of using CrewAI is that it templatizes parameters so that destination, flight, hotel location, trip_duration, personal_interests, dining_restrictions_preferences and email are all passing as individual template variables rather than mushed together as an agent's user prompt. One disadvantage of CrewAI is that individual task completion is invisible.  Therefore, I cannot display the progress in UI.  I cannot get the final output of individual task either.
* [Engineer Team Powered by CrewAI](https://huggingface.co/spaces/threecuptea/engineer_team): Engineer team is composed of engineer_lead, backend_engineer, frontend_engineer and test_engineer. Given the class_name, module_name and requirements, engineer_lead will layout all interface function calls in Markdown form and backend_engineer work on coding for the class followed by coding by frontend engineer and test_engineer. The  end product is an Account Management application of Trading Simulation Platform.
* [Sidekick](https://huggingface.co/spaces/threecuptea/sidekick-docker): it's app of Langgraph framework with conditional routing and evaluator feedback loop that utilize rich langchain-community resources like GoogleSerper, Playwright browser tools etc.;Huggingface docker deployment with Chromium.
  The Sidekick can assist a lot of issue.  However, I also found a couple of issues in the process.
  For gpt-4o-mini, if I asked 'which MLB teams won consecutive world series champions as of now since 1990', it always stated that it can only return information for sure as of 2023 and did not include LA Dodger 2024-2025 back-to-back champion. For gpt-4.1-mini, if I asked 'which MLB teams won consecutive world series champions as of December 24, 2025 since 1990', it sometimes insisted that there is no team winning champion since 2023, hereby did not include LA Dodger. It sometimes return the correct answer.
  If I asked 'fact check president trump's prime-time speech in December 17, 2025' without any site hint, gpt-4.1-mini will ask site hints and gpt-4o-mini have never asked any questions and sometime hit recursion-limit 25.  It sometimes would navigate or serach_element of non-existent site and got an error and got stuck.  If I frame the question well like 'fact check president trump's prime-time speech in December 17, 2025, suggest using [cnn.com](http://cnn.com/) or [factcheck.org](http://factcheck.org/), it will stay on track and deliver good result.
* [langgraph_checkpoint_howto](https://github.com/threecuptea/agentic_ai/tree/master/langgraph_checkpoint_howto) is my research on how to dump Langgraph StateSnapshots, retrieve the context (useful information) of each StateSnapshot and re-run using the checkpoint_id.  As I mentioned that `checkpoint` is one of reason that I become a true believer of Langgraph. Breakpoint help repeat isolated testing and cut down the cost.
* Ed's capstone 'trader' project is a fantastic/ creative project.  It utilizes 6 MCP servers & 41 tools when codes using the pay version of Polygon API and 6 MCP servers & 16 tools when codes using the free version. It simulates 4 famous traders: Warren Buffett, George Soros, Ray Dalio and Cathie Wood who trade utilizing their characteristic strategies.  It has production-grade sleek Gradio UI and it is using best practice: prompts, instructions and MCP params are all configured externally instead of hard-coding.  It's using async (non-blocking) operation.  However, it does have a long hanging bugs in codes using the free version of Polygon API. A couple of people attempted to fix it previously. I finally figure out why in [My Pull Request 533](https://github.com/ed-donner/agents/pull/533).  It got "NOT_AUTHORIZED" error from Polygon API if running 'trader' when it's one day ahead in UTC time than New York time due to `get_previous_close_agg` using UTC time cut-off and 'get\_grouped\_daily\_aggs' using a trading date which is NYC time.  The details is in README.md of fix_polygon_not_authorized_error.
* [Agentic RAG](https://github.com/threecuptea/agentic_ai/tree/master/sonya_agentic_rag_v2) is my first RAG system. The Agentic RAG is equipped w/ Chunking, Hybrid Retriever and MRR (Re-rank).  It simulates an intelligent RAG system (Q & A Assistant) in Health Care Industry. It uses Langraph workflow equipped with Routing Agent, Relevance Checker, Fallback web search and answer generator of course.  It provides two retrieval modes: a Simple Semantic Retriever and a Hybrid Retriever with Re-ranker.  The former is using ChromaDB collection query combined with Cosine distance filter. The latter retrieves a larger pool of candidates by formulating weighted normalized semantic and keyword scores then re-rank to choose top-k based upon how candidates are related to the query.  I document what I discovered and learned from the hand-on experience and what I can enhance in the future RAG projects in details in the README of the project.
